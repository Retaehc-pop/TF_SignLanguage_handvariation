{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA preprocessing and create lable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as pyplot\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"DATA\") # paht of the exported data\n",
    "alphabet = string.ascii_lowercase+string.digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "character,lables = [], []\n",
    "alphabet = np.array(os.listdir(os.path.join(DATA_PATH)))\n",
    "label_map = {lable:num for num,lable in enumerate(alphabet)}\n",
    "for char in alphabet:\n",
    "  for files in os.listdir(os.path.join(DATA_PATH,char)):\n",
    "    res = np.load(os.path.join(DATA_PATH,char,files))\n",
    "    character.append(res)\n",
    "    lables.append(label_map[char])\n",
    "\n",
    "X = np.array(character)\n",
    "y = to_categorical(lables).astype(int)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "114/114 [==============================] - 1s 9ms/step - loss: 2.6892 - categorical_accuracy: 0.0784\n",
      "Epoch 2/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 2.5206 - categorical_accuracy: 0.1414\n",
      "Epoch 3/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 2.2361 - categorical_accuracy: 0.2091\n",
      "Epoch 4/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 2.0179 - categorical_accuracy: 0.2696\n",
      "Epoch 5/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 1.7931 - categorical_accuracy: 0.3535\n",
      "Epoch 6/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 1.5544 - categorical_accuracy: 0.4206\n",
      "Epoch 7/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 1.3805 - categorical_accuracy: 0.4768\n",
      "Epoch 8/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 1.2623 - categorical_accuracy: 0.5161\n",
      "Epoch 9/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 1.1114 - categorical_accuracy: 0.5590\n",
      "Epoch 10/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 1.0029 - categorical_accuracy: 0.6168\n",
      "Epoch 11/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.9225 - categorical_accuracy: 0.6440\n",
      "Epoch 12/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.8667 - categorical_accuracy: 0.6622\n",
      "Epoch 13/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.8256 - categorical_accuracy: 0.6784\n",
      "Epoch 14/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.7659 - categorical_accuracy: 0.6966\n",
      "Epoch 15/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.7099 - categorical_accuracy: 0.7230\n",
      "Epoch 16/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.6787 - categorical_accuracy: 0.7376\n",
      "Epoch 17/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.6499 - categorical_accuracy: 0.7411\n",
      "Epoch 18/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.6156 - categorical_accuracy: 0.7579\n",
      "Epoch 19/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.5796 - categorical_accuracy: 0.7805\n",
      "Epoch 20/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.5401 - categorical_accuracy: 0.8025\n",
      "Epoch 21/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.5114 - categorical_accuracy: 0.8116\n",
      "Epoch 22/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4985 - categorical_accuracy: 0.8165\n",
      "Epoch 23/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4864 - categorical_accuracy: 0.8212\n",
      "Epoch 24/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4581 - categorical_accuracy: 0.8374\n",
      "Epoch 25/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4634 - categorical_accuracy: 0.8286\n",
      "Epoch 26/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4132 - categorical_accuracy: 0.8495\n",
      "Epoch 27/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4253 - categorical_accuracy: 0.8479\n",
      "Epoch 28/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4214 - categorical_accuracy: 0.8399\n",
      "Epoch 29/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4104 - categorical_accuracy: 0.8528\n",
      "Epoch 30/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4247 - categorical_accuracy: 0.8437\n",
      "Epoch 31/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.4075 - categorical_accuracy: 0.8564\n",
      "Epoch 32/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3784 - categorical_accuracy: 0.8594\n",
      "Epoch 33/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3560 - categorical_accuracy: 0.8814\n",
      "Epoch 34/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3713 - categorical_accuracy: 0.8635\n",
      "Epoch 35/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3423 - categorical_accuracy: 0.8798\n",
      "Epoch 36/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3677 - categorical_accuracy: 0.8680\n",
      "Epoch 37/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3370 - categorical_accuracy: 0.8806\n",
      "Epoch 38/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3504 - categorical_accuracy: 0.8699\n",
      "Epoch 39/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3314 - categorical_accuracy: 0.8817\n",
      "Epoch 40/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3772 - categorical_accuracy: 0.8655\n",
      "Epoch 41/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3398 - categorical_accuracy: 0.8814\n",
      "Epoch 42/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3052 - categorical_accuracy: 0.8938\n",
      "Epoch 43/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2874 - categorical_accuracy: 0.8968\n",
      "Epoch 44/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3028 - categorical_accuracy: 0.8908\n",
      "Epoch 45/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3137 - categorical_accuracy: 0.8927\n",
      "Epoch 46/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.3284 - categorical_accuracy: 0.8891\n",
      "Epoch 47/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2871 - categorical_accuracy: 0.8999\n",
      "Epoch 48/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2830 - categorical_accuracy: 0.9010\n",
      "Epoch 49/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2730 - categorical_accuracy: 0.9062\n",
      "Epoch 50/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2781 - categorical_accuracy: 0.9045\n",
      "Epoch 51/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.3024 - categorical_accuracy: 0.8968\n",
      "Epoch 52/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2472 - categorical_accuracy: 0.9186\n",
      "Epoch 53/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2963 - categorical_accuracy: 0.8935\n",
      "Epoch 54/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2693 - categorical_accuracy: 0.9048\n",
      "Epoch 55/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2625 - categorical_accuracy: 0.9106\n",
      "Epoch 56/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2601 - categorical_accuracy: 0.9067\n",
      "Epoch 57/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2564 - categorical_accuracy: 0.9081\n",
      "Epoch 58/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2502 - categorical_accuracy: 0.9186\n",
      "Epoch 59/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2810 - categorical_accuracy: 0.9098\n",
      "Epoch 60/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2503 - categorical_accuracy: 0.9153: 0s - loss: 0.3337 - categorical_accu\n",
      "Epoch 61/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2605 - categorical_accuracy: 0.9142\n",
      "Epoch 62/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2505 - categorical_accuracy: 0.9131\n",
      "Epoch 63/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2575 - categorical_accuracy: 0.9125\n",
      "Epoch 64/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2169 - categorical_accuracy: 0.9241\n",
      "Epoch 65/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2201 - categorical_accuracy: 0.9279\n",
      "Epoch 66/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2652 - categorical_accuracy: 0.9070\n",
      "Epoch 67/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2338 - categorical_accuracy: 0.9197\n",
      "Epoch 68/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2347 - categorical_accuracy: 0.9199\n",
      "Epoch 69/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2237 - categorical_accuracy: 0.9265\n",
      "Epoch 70/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2486 - categorical_accuracy: 0.9073\n",
      "Epoch 71/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2310 - categorical_accuracy: 0.9208\n",
      "Epoch 72/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2324 - categorical_accuracy: 0.9199\n",
      "Epoch 73/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2422 - categorical_accuracy: 0.9161\n",
      "Epoch 74/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2009 - categorical_accuracy: 0.9345\n",
      "Epoch 75/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2021 - categorical_accuracy: 0.9301\n",
      "Epoch 76/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2419 - categorical_accuracy: 0.9194\n",
      "Epoch 77/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2394 - categorical_accuracy: 0.9235\n",
      "Epoch 78/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1876 - categorical_accuracy: 0.9392\n",
      "Epoch 79/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1755 - categorical_accuracy: 0.9433\n",
      "Epoch 80/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2084 - categorical_accuracy: 0.9285\n",
      "Epoch 81/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1987 - categorical_accuracy: 0.9359\n",
      "Epoch 82/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2067 - categorical_accuracy: 0.9296\n",
      "Epoch 83/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1954 - categorical_accuracy: 0.9365\n",
      "Epoch 84/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2093 - categorical_accuracy: 0.9329\n",
      "Epoch 85/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2137 - categorical_accuracy: 0.9298\n",
      "Epoch 86/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2271 - categorical_accuracy: 0.9221\n",
      "Epoch 87/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2054 - categorical_accuracy: 0.9296\n",
      "Epoch 88/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2194 - categorical_accuracy: 0.9238\n",
      "Epoch 89/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1921 - categorical_accuracy: 0.9365\n",
      "Epoch 90/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1617 - categorical_accuracy: 0.9483\n",
      "Epoch 91/100\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 0.1960 - categorical_accuracy: 0.9356\n",
      "Epoch 92/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1786 - categorical_accuracy: 0.9414\n",
      "Epoch 93/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1672 - categorical_accuracy: 0.9458\n",
      "Epoch 94/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1689 - categorical_accuracy: 0.9458\n",
      "Epoch 95/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.2068 - categorical_accuracy: 0.9323\n",
      "Epoch 96/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1663 - categorical_accuracy: 0.9464\n",
      "Epoch 97/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1772 - categorical_accuracy: 0.9436\n",
      "Epoch 98/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1681 - categorical_accuracy: 0.9439\n",
      "Epoch 99/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1631 - categorical_accuracy: 0.9461\n",
      "Epoch 100/100\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 0.1505 - categorical_accuracy: 0.9505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19b91c1aa88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64,activation=\"tanh\",input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(alphabet.shape[0],activation=\"softmax\"))\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "model.fit(X_train,y_train,epochs=100,callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 64)                4096      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 15)                975       \n",
      "=================================================================\n",
      "Total params: 9,263\n",
      "Trainable params: 9,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"action.h5\")\n",
    "tfjs.converters.save_keras_model(model, \"tfjs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9635416666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix,accuracy_score\n",
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test,axis=1).tolist()\n",
    "yhat = np.argmax(yhat,axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue,yhat)\n",
    "accuracy_score(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities\n",
    "\n",
    "def mediapipe_detection(image,model):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # color conversion\n",
    "  image = cv2.flip(image, 1)\n",
    "  image.flags.writeable = False\n",
    "  results = model.process(image) # process image\n",
    "  image.flags.writeable = True\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # color conversion\n",
    "  return image,results\n",
    "\n",
    "def draw_landmarks(image,result):\n",
    "  if result.multi_hand_landmarks:\n",
    "    for num,hand in enumerate(result.multi_hand_landmarks):\n",
    "      mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(0, 112, 255), thickness=1, circle_radius=2),\n",
    "                                mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=2, circle_radius=4),\n",
    "                                )\n",
    "def extract_keypoint(result):\n",
    "  if result.multi_hand_landmarks:\n",
    "    for hand_landmarks in result.multi_hand_landmarks:\n",
    "      all_hand_pos = []\n",
    "      for joint in mp_hands.HandLandmark:\n",
    "        all_hand_pos.append(np.array([hand_landmarks.landmark[joint].x,hand_landmarks.landmark[joint].y,hand_landmarks.landmark[joint].z]))\n",
    "  else:\n",
    "    return np.zeros(21*3)\n",
    "  return np.concatenate(all_hand_pos) # return 63 point of connection\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245),(44,217,235)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9060/337696120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Make detections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmediapipe_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Draw landmarks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdraw_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9060/3719345802.py\u001b[0m in \u001b[0;36mmediapipe_detection\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m   \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# process image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m   \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2BGR\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# color conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf37\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \"\"\"\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf37\\lib\\site-packages\\mediapipe\\python\\solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    332\u001b[0m                                      data).at(self._simulated_timestamp))\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_until_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m     \u001b[1;31m# Create a NamedTuple object where the field names are mapping to the graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;31m# output stream names.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "word = []\n",
    "threshold = 0.9\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5,max_num_hands=1) as hands:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoint(results)\n",
    "        sequence = keypoints\n",
    "        res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        if res[np.argmax(res)] > threshold: \n",
    "            if len(word)>0:\n",
    "                if alphabet[np.argmax(res)] != word[-1]:\n",
    "                    word.append(alphabet[np.argmax(res)])\n",
    "            else:\n",
    "                word.append(alphabet[np.argmax(res)])\n",
    "\n",
    "        if len(word) > 10: \n",
    "                word = word[-10:]\n",
    "        \n",
    "        # Viz probabilities\n",
    "        # image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(word), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9103b89518c9592aaf7b51401b88ec12ced5e1cde478347da803a0f3bed09c08"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tf37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
